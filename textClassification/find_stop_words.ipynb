{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'VNTC_data/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertData(file_path):\n",
    "    X = []\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = ' '.join(lines)\n",
    "        # remove some specical characters\n",
    "        lines = gensim.utils.simple_preprocess(lines)\n",
    "        lines - ' '.join(lines)\n",
    "        # tokenize\n",
    "        lines = ViTokenizer.tokenize(lines)\n",
    "        X.append(lines)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(folder_name):\n",
    "    for folder in os.listdir(folder_name):\n",
    "        class_path = os.path.join(folder_name, folder)\n",
    "        print(class_path)\n",
    "        X = []\n",
    "        for j in tqdm(os.listdir(class_path)):\n",
    "            file_path = os.path.join(class_path, j)\n",
    "            X += convertData(file_path)\n",
    "    # save file\n",
    "        pickle.dump(X, open(\"VNTC_data/data_pkl/{}.pkl\".format(folder), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VNTC_data/Train_Full\\Chinh tri Xa hoi\n",
      "VNTC_data/Train_Full\\Doi song\n",
      "VNTC_data/Train_Full\\Khoa hoc\n",
      "VNTC_data/Train_Full\\Kinh doanh\n",
      "VNTC_data/Train_Full\\Phap luat\n",
      "VNTC_data/Train_Full\\Suc khoe\n",
      "VNTC_data/Train_Full\\The gioi\n",
      "VNTC_data/Train_Full\\The thao\n",
      "VNTC_data/Train_Full\\Van hoa\n",
      "VNTC_data/Train_Full\\Vi tinh\n"
     ]
    }
   ],
   "source": [
    "findPath(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (python list)\n",
    "X_data = pickle.load(open('VNTC_data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('VNTC_data/y_data.pkl', 'rb'))\n",
    "X_test = pickle.load(open('VNTC_data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('VNTC_data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findIndex(label):\n",
    "    index = [0]\n",
    "    ind = 0\n",
    "    for name in sorted(set(label)):\n",
    "        ind += label.count(name)\n",
    "        # print(name, ind)\n",
    "        index.append(ind)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5219, 8378, 10198, 12750, 16618, 20002, 22900, 28198, 31278, 33759]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findIndex(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7567, 9603, 11699, 16975, 20763, 26180, 32896, 39563, 45813, 50373]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findIndex(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5219\n",
      "8378\n",
      "10198\n",
      "12750\n",
      "16618\n",
      "20002\n",
      "22900\n",
      "28198\n",
      "31278\n"
     ]
    }
   ],
   "source": [
    "index_data = findIndex(y_data)\n",
    "index_test = findIndex(y_test)\n",
    "for i in range(len(index_data) - 1):\n",
    "    print(index_data[i])\n",
    "    name = set(y_data[index_data[i]:index_data[i+1]] + y_test[index_test[i]:index_test[i+1]])\n",
    "    data = X_data[index_data[i]:index_data[i+1]] + X_test[index_test[i]:index_test[i+1]]\n",
    "    pickle.dump(data, open(\"VNTC_data/data/{}.pkl\".format(name.pop()), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def createStopwords(data, max_features):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=max_features)\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveStopwords(max_features):\n",
    "    stopwords = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        path_file = os.path.join(data_dir, file)\n",
    "        data = pickle.load(open(path_file, 'rb'))\n",
    "        stopwords.append(createStopwords(data, max_features))\n",
    "    # find intersection \n",
    "    intersection = stopwords[0]\n",
    "    for i in stopwords:\n",
    "        intersection = (i & intersection)\n",
    "    print(\"Number of stopwords\", len(intersection))\n",
    "    \n",
    "    # write to text\n",
    "    with open('stopwords_tfidf_{}.txt'.format(max_features), 'w+', encoding='utf-8') as find_stopwords_file:\n",
    "        for i in range(len(intersection)):\n",
    "            find_stopwords_file.write(intersection.pop() + '\\n')\n",
    "        find_stopwords_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords 10727\n"
     ]
    }
   ],
   "source": [
    "saveStopwords(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords 2102\n"
     ]
    }
   ],
   "source": [
    "saveStopwords(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords 177\n"
     ]
    }
   ],
   "source": [
    "saveStopwords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords 3\n"
     ]
    }
   ],
   "source": [
    "saveStopwords(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
