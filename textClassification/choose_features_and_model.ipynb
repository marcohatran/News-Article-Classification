{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using VNTC-data\n",
    "# https://github.com/duyvuleo/VNTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and POS tagger\n",
    "from pyvi import ViTokenizer, ViPosTagger \n",
    "from underthesea import word_tokenize, pos_tag\n",
    "# progress bar\n",
    "from tqdm.notebook import tqdm \n",
    "# save and load sklearn models\n",
    "from joblib import dump, load \n",
    "# simple processing of text: remove special characters, numberic characters\n",
    "import gensim \n",
    "# path\n",
    "import os \n",
    "# save raw data text file \n",
    "import pickle  \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# data manipulation and models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# metrics\n",
    "from sklearn import metrics\n",
    "# save and load model\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [RidgeClassifier(), SGDClassifier(), Perceptron(), PassiveAggressiveClassifier(), LogisticRegression() # linear \n",
    "         , BernoulliNB(), ComplementNB(), MultinomialNB()                                                       # naive bayes\n",
    "         , LinearSVC()                                                                                          # SVM\n",
    "         , RandomForestClassifier()                                                                             # ensemble\n",
    "         , XGBClassifier()                                                                                      # boosting\n",
    "         ]                                                                                          \n",
    "model_names = ['RidgeClassifier', 'SGDClassifier', 'Perceptron', 'PassiveAggressiveClassifier', 'LogisticRegression'\n",
    "              , 'BernoulliNB', 'ComplementNB', 'MultinomialNB'\n",
    "              , 'LinearSVC'\n",
    "              , 'RandomForestClassifier'\n",
    "              , 'XGBClassifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'VNTC_data'\n",
    "def get_data(folder_path):\n",
    "    X = [] \n",
    "    y = []\n",
    "    dirs = os.listdir(folder_path)\n",
    "    for path in tqdm(dirs):\n",
    "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
    "        for file_path in tqdm(file_paths):\n",
    "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
    "                lines = f.readlines()\n",
    "                lines = ' '.join(lines)\n",
    "                # remove some special characters\n",
    "                lines = gensim.utils.simple_preprocess(lines)\n",
    "                lines = ' '.join(lines)\n",
    "                # tokenizer\n",
    "                lines = ViTokenizer.tokenize(lines)\n",
    "                # text data \n",
    "                X.append(lines)\n",
    "                # labels\n",
    "                y.append(path)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc9bbcbd2de4359aeff87bf36f5590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d283eb2b9b604ec4bae1e8eff9dc53d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5219.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b942ec57f3f04ff38f6e138be65c8a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-cc819d92357d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Train_Full'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# save raw training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VNTC_data/X_data.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VNTC_data/y_data.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-771330a7ec1d>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mfile_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-16\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\encodings\\utf_16.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBufferedIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBufferedIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(dir_path, 'Train_Full')\n",
    "X_data, y_data = get_data(train_path)\n",
    "# save raw training data\n",
    "pickle.dump(X_data, open('VNTC_data/X_data.pkl', 'wb'))\n",
    "pickle.dump(y_data, open('VNTC_data/y_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dir_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-afacee14aee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Test_Full'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VNTC_data/X_test.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VNTC_data/y_test.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dir_path' is not defined"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(dir_path, 'Test_Full')\n",
    "X_test, y_test = get_data(test_path)\n",
    "# save raw testing data\n",
    "pickle.dump(X_test, open('VNTC_data/X_test.pkl', 'wb'))\n",
    "pickle.dump(y_test, open('VNTC_data/y_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (python list)\n",
    "X_data = pickle.load(open('VNTC_data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('VNTC_data/y_data.pkl', 'rb'))\n",
    "X_test = pickle.load(open('VNTC_data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('VNTC_data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33759\n",
      "50373\n",
      "84132\n"
     ]
    }
   ],
   "source": [
    "print(len(X_data))\n",
    "print(len(X_test))\n",
    "print(len(X_data + X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1942\n"
     ]
    }
   ],
   "source": [
    "# import list of stopwords\n",
    "stop_words = []\n",
    "with open('vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        stop_words.append(line.rstrip().strip())\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044\n"
     ]
    }
   ],
   "source": [
    "with open('stopwords_tfidf_50000.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        stop_words.append(line.rstrip().strip())\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVectorizerData(X_data, X_test, stop_words, ngram_range, max_features):\n",
    "    tfidf_vect = TfidfVectorizer(lowercase=False, analyzer='word'\n",
    "                                ,stop_words=stop_words, ngram_range=ngram_range\n",
    "                                ,max_features=max_features)\n",
    "    # later\n",
    "    # norm\n",
    "    # sublinear_tf\n",
    "    # \n",
    "    tfidf_vect.fit(X_data)\n",
    "    # transform \n",
    "    X_data_tfidf = tfidf_vect.transform(X_data)\n",
    "    X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "    X = tfidf_vect.transform(X_data + X_test)\n",
    "    return (tfidf_vect, X_data_tfidf, X_test_tfidf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(classifier, data, labels, train_data, train_labels, test_data, test_labels, stats_name, model_name):\n",
    "    # split to training and validation data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.2, random_state=21)\n",
    "    \n",
    "    # train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # build the stats report\n",
    "        # on the training data\n",
    "    train_pred = classifier.predict(train_data)\n",
    "    train_clf_rp = metrics.classification_report(train_labels, train_pred)\n",
    "    train_cfs_rp = metrics.confusion_matrix(train_labels, train_pred, labels=classifier.classes_)\n",
    "        # on the testing data\n",
    "    test_pred = classifier.predict(test_data)\n",
    "    test_clf_rp = metrics.classification_report(test_labels, test_pred)\n",
    "    test_cfs_rp = metrics.confusion_matrix(test_labels, test_pred, labels=classifier.classes_)\n",
    "        # on the whole data\n",
    "    pred = classifier.predict(data)\n",
    "    clf_rp = metrics.classification_report(labels, pred)\n",
    "    cfs_rp = metrics.confusion_matrix(labels, pred, labels=classifier.classes_)\n",
    "        # save the report \n",
    "    with open('report//%s.txt' %stats_name, 'w', encoding='utf-8') as report_file:\n",
    "        report_file.write('Training data:\\n')\n",
    "        report_file.write(train_clf_rp)\n",
    "        report_file.write(np.array2string(train_cfs_rp, separator = ','))\n",
    "        \n",
    "        report_file.write('\\n\\nTesting data:\\n')\n",
    "        report_file.write(test_clf_rp)\n",
    "        report_file.write(np.array2string(test_cfs_rp, separator = ','))\n",
    "        \n",
    "        report_file.write('\\n\\nWhole data:\\n')\n",
    "        report_file.write(clf_rp)\n",
    "        report_file.write(np.array2string(cfs_rp, separator = ','))\n",
    "        \n",
    "        report_file.close()\n",
    "    # save the model \n",
    "    dump(classifier, 'models//%s.joblib' %model_name)\n",
    "    # print the accuracy (for choosing model)\n",
    "    print('Testing data accuracy: ', metrics.accuracy_score(test_pred, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tfidf unigram to choose the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_11, X_test_tfidf_11, X_tfidf_11) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(1, 1), \n",
    "                                                                         max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier\n",
      "Testing data accuracy:  0.9112421336827269\n",
      "SGDClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.9147162170210231\n",
      "Perceptron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.8914299327020427\n",
      "PassiveAggressiveClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in PassiveAggressiveClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.9066166398665952\n",
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.9152720703551506\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(linear_model):\n",
    "    print(linear_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_11, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_11, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_11, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_11' %linear_model_names[i]\n",
    "               , model_name = '%s_tfidf_11' %linear_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB\n",
      "Testing data accuracy:  0.8687789093363508\n",
      "ComplementNB\n",
      "Testing data accuracy:  0.8810870903063149\n",
      "MultinomialNB\n",
      "Testing data accuracy:  0.8906358565104322\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(naive_bayes_model):\n",
    "    print(naive_bayes_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_11, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_11, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_11, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_11' %naive_bayes_model_names[i]\n",
    "               , model_name = '%s_tfidf_11' %naive_bayes_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "Testing data accuracy:  0.9160264427371806\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(SVM_model):\n",
    "    print(SVM_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_11, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_11, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_11, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_11' %SVM_model_names[i]\n",
    "               , model_name = '%s_tfidf_11' %SVM_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.8214519683163599\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(ensemble_model):\n",
    "    print(ensemble_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_11, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_11, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_11, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_11' %ensemble_model_names[i]\n",
    "               , model_name = '%s_tfidf_11' %ensemble_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier\n",
      "Testing data accuracy:  0.8963532050900284\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(xgboost_model):\n",
    "    print(xgboost_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_11, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_11, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_11, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_11' %xgboost_model_names[i]\n",
    "               , model_name = '%s_tfidf_11' %xgboost_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose 2 models: logisticRegression - 0.9152720703551506, linearSVC - 0.9160264427371806. Testing 2 models with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosed_model = [LogisticRegression(), LinearSVC()]\n",
    "choosed_model_names = ['LogisticRegression', 'LinearSVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_22, X_test_tfidf_22, X_tfidf_22) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(2, 2), \n",
    "                                                                         max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.8516268635975622\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.8472792964484942\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(choosed_model):\n",
    "    print(choosed_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_22, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_22, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_22, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_22' %choosed_model_names[i]\n",
    "               , model_name = '%s_tfidf_22' %choosed_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need more features because bigram have a lot of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_22, X_test_tfidf_22, X_tfidf_22) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(2, 2), \n",
    "                                                                         max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Testing data accuracy:  0.8626843745657395\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.8625255593274175\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(choosed_model):\n",
    "    print(choosed_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_22, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_22, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_22, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_22' %choosed_model_names[i]\n",
    "               , model_name = '%s_tfidf_22' %choosed_model_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_22, X_test_tfidf_22, X_tfidf_22) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(2, 2), \n",
    "                                                                         max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Testing data accuracy:  0.869612689337542\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.8785063426835805\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(choosed_model):\n",
    "    print(choosed_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_22, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_22, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_22, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_22' %choosed_model_names[i]\n",
    "               , model_name = '%s_tfidf_22' %choosed_model_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_22, X_test_tfidf_22, X_tfidf_22) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(2, 2), \n",
    "                                                                         max_features=70000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Testing data accuracy:  0.8704067655291525\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.8825561312607945\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(choosed_model):\n",
    "    print(choosed_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_22, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_22, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_22, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_22' %choosed_model_names[i]\n",
    "               , model_name = '%s_tfidf_22' %choosed_model_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_22, X_test_tfidf_22, X_tfidf_22) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(2, 2), \n",
    "                                                                         max_features=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Testing data accuracy:  0.8705457288626843\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.8864272526948961\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(choosed_model):\n",
    "    print(choosed_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_22, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_22, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_22, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_22' %choosed_model_names[i]\n",
    "               , model_name = '%s_tfidf_22' %choosed_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining unigram and bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, X_data_tfidf_12, X_test_tfidf_12, X_tfidf_12) = createVectorizerData(X_data, X_test, \n",
    "                                                                         stop_words, \n",
    "                                                                         ngram_range=(1, 2), \n",
    "                                                                         max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Testing data accuracy:  0.9155698489270045\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.9178131141683045\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(choosed_model):\n",
    "    print(choosed_model_names[i])\n",
    "    trainModel(classifier = model\n",
    "               , data=X_tfidf_12, labels = y_data + y_test\n",
    "               , train_data = X_data_tfidf_12, train_labels = y_data\n",
    "               , test_data = X_test_tfidf_12, test_labels = y_test\n",
    "               , stats_name = '%s_tfidf_12' %choosed_model_names[i]\n",
    "               , model_name = '%s_tfidf_12' %choosed_model_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxFeatures(max_features):\n",
    "    print('\\n Max Features: ', max_features)\n",
    "    # build vectorizer object for tfidf features\n",
    "    (vectorizer, X_data_tfidf_12, X_test_tfidf_12, X_tfidf_12) = createVectorizerData(X_data, X_test, \n",
    "                                                                                      stop_words, \n",
    "                                                                                      ngram_range=(1, 2), \n",
    "                                                                                      max_features=max_features)\n",
    "    # save infomation about features for future opimization\n",
    "    pickle.dump(vectorizer, open(\"features/vectorizer_tfidf_12_%d\" %max_features, \"wb\"))\n",
    "    # train choosed models and print accuracy on test set\n",
    "    for i, model in enumerate(choosed_model):\n",
    "        print(choosed_model_names[i])\n",
    "        trainModel(classifier = model\n",
    "                   , data=X_tfidf_12, labels = y_data + y_test\n",
    "                   , train_data = X_data_tfidf_12, train_labels = y_data\n",
    "                   , test_data = X_test_tfidf_12, test_labels = y_test\n",
    "                   , stats_name = '%s_tfidf_12_%d' %(choosed_model_names[i], max_features)\n",
    "                   , model_name = '%s_tfidf_12_%d' %(choosed_model_names[i], max_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  10000\n",
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy:  0.915450737498263\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.9159271832132293\n",
      "\n",
      " Max Features:  30000\n",
      "LogisticRegression\n",
      "Testing data accuracy:  0.9151728108311993\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.9187461536934469\n",
      "\n",
      " Max Features:  50000\n",
      "LogisticRegression\n",
      "Testing data accuracy:  0.9153713298791019\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.9190836360748814\n",
      "\n",
      " Max Features:  70000\n",
      "LogisticRegression\n",
      "Testing data accuracy:  0.9150934032120382\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.9193814146467354\n",
      "\n",
      " Max Features:  90000\n",
      "LogisticRegression\n",
      "Testing data accuracy:  0.9149742917832966\n",
      "LinearSVC\n",
      "Testing data accuracy:  0.9197387489329601\n"
     ]
    }
   ],
   "source": [
    "for i in [10000, 30000, 50000, 70000, 90000]:\n",
    "    findMaxFeatures(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Choosing LinearSVC as main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  100000\n",
      "Testing data accuracy:  0.9197784527425407\n",
      "\n",
      " Max Features:  200000\n",
      "Testing data accuracy:  0.9201357870287654\n",
      "\n",
      " Max Features:  30000\n",
      "Testing data accuracy:  0.9187461536934469\n"
     ]
    }
   ],
   "source": [
    "for i in [100000, 200000, 30000]:\n",
    "    findMaxFeatures(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  300000\n",
      "Testing data accuracy:  0.9204931213149902\n",
      "\n",
      " Max Features:  500000\n",
      "Testing data accuracy:  0.9205328251245707\n",
      "\n",
      " Max Features:  1000000\n",
      "Testing data accuracy:  0.9210291227443274\n"
     ]
    }
   ],
   "source": [
    "for i in [300000, 500000, 1000000]:\n",
    "    findMaxFeatures(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  None\n",
      "Testing data accuracy:  0.9208703075060052\n"
     ]
    }
   ],
   "source": [
    "findMaxFeatures(None) # mean maximum of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    some optimizations: collect more data, finding opimized value for number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxFeatures(max_features):\n",
    "    print('\\n Max Features: ', max_features)\n",
    "    # build vectorizer object for tfidf features\n",
    "    (vectorizer, X_data_tfidf_12, X_test_tfidf_12, X_tfidf_12) = createVectorizerData(X_data, X_test, \n",
    "                                                                                      stop_words=stop_words, \n",
    "                                                                                      ngram_range=(1, 2), \n",
    "                                                                                      max_features=max_features)\n",
    "    # save infomation about features for future opimization\n",
    "    pickle.dump(vectorizer, open(\"features/vectorizer_tfidf_12___{}\".format(max_features), \"wb\"))\n",
    "    # train choosed models and print accuracy on test set\n",
    "    \n",
    "    # l1 work better for spare data and l2 work better for non-spare cases\n",
    "    trainModel(classifier = LinearSVC(penalty='l1', loss='l2', dual=False, max_iter=2000, tol=1e-05, class_weight='balanced')\n",
    "                , data=X_tfidf_12, labels = y_data + y_test\n",
    "                , train_data = X_data_tfidf_12, train_labels = y_data\n",
    "                , test_data = X_test_tfidf_12, test_labels = y_test\n",
    "                , stats_name = 'LinearSVC_tfidf_12___{}'.format(max_features)\n",
    "                , model_name = 'LinearSVC_tfidf_12___{}'.format(max_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC(penalty='l1', loss='l2', dual=False, max_iter=2000, tol=1e-05, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hhd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\classes.py:220: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "findMaxFeatures(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  100000\n",
      "Testing data accuracy:  0.9198578603617017\n"
     ]
    }
   ],
   "source": [
    "findMaxFeatures(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Max Features:  200000\n",
      "Testing data accuracy:  0.9201754908383459\n"
     ]
    }
   ],
   "source": [
    "findMaxFeatures(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
